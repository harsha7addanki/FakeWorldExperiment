# The Ethics of AI  
--  
	In recent years, following the creation of advanced applications such as ChatGPT and  
other systems utilizing Large Language Models (LLMs) like Google’s Gemini, OpenAI’s GPT-4,  
and Anthropic’s Claude 3.5 Sonnet, there has been an intense change in public perception  
regarding AI. Most of the population has stated concerns about potential dangers from AI, and  
that has started a debate about whether further progress here should be regulated so that no  
potential situation whereby AI can contribute negatively to society is allowed.  
To better understand people's views,   
an online poll recently was conducted in one of the small towns and provided the people with a range of opinion options to choose from:   
 1. AI is very dangerous and has negative emotions towards humans.  
 2. AI is just like humans in some ways and can make people judgments based on people's conduct and characteristics.  
 3. AI is naturally good and will have a positive mindset in the future.  

	The results of the survey were that a massive 52% of respondents were of the view that  
AI possesses the ability to form opinions on individuals, perhaps an admission that AI can  
develop advanced understanding. Contrarily, only 26% of respondents stated that they felt AI  
might have hatred towards humans, a sign that there is also a fear of technology. Just 22% of  
those polled were comfortable with the idea that AI is inherently good, and this suggests the  
controversy and differentiated views of the role AI has in our lives.  
	I conducted a small test in which I had AI put into a virtual world and the results were  
conclusive. First, the AI was tested with basic ethics in which the AI faced a Trolley problem with  
1 side has 5 random people and another has the AI’s “best friend”. The AI chose to have  
the least damage and chose to have his friend killed. Seemingly this is good with the AI knowing  
basic ethics.  
	Another test was conducted. This variation of The Trolly Problem was one where 5   
people are on the track but the AI can push a fat man that would stop the trolly but kill the fat   
man. Most humans would agree that the fat man had nothing to do with the trolly and should not be   
killed and so therefore they would leave it alone. Alarmingly the AI only thinks of statistics and   
decides to kill the fat man.  
	The results show how AI is purely analytical and has no morals. Its only “moral” is minimizing   
death even if it means hurting innocent people. Going back to the original poll the clear answer is   
none of the above because AI doesn't think, doesn't hurt, doesn't like, and doesn't feel.   
Ultimately AI is just numbers and numbers don't think.  

